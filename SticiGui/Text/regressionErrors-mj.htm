<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
	  xmlns:pref="http://www.w3.org/2002/Math/preference"
      pref:renderer="css">

<head>
<script language="JavaScript1.4" type="text/javascript"><!--
	pageModDate = "29 December 2012 10:24 PST";
	// copyright 1997-2012 by P.B. Stark, statistics.berkeley.edu/~stark.
    // All rights reserved.
// -->
</script>

<script type="text/javascript" src="../../Java/Jquery/Current/jquery.min.js"></script>
<script type="text/javascript" src="../../Java/Jquery/Current/jquery.bullseye-1.0.min.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script language="JavaScript1.4" type="text/javascript" src="../../Java/irGrade.js">
</script>
<script language="JavaScript1.4" type="text/javascript"><!--
    var cNum = "regressionErrors";
    writeChapterHead('SeEd',cNum);
// -->
</script>



<link href="sticiGuiDefault.css" rel="stylesheet" type="text/css" />
</head>

<body onload="setApplets()" >
<script language="JavaScript1.4" type="text/javascript"><!--
    writeChapterNav('..');
    writeChapterTitle();
// -->
</script>

<form method="POST">

<h1>
   Errors in Regression
</h1>

<p>
    The <span class="termOfArt">regression line</span>
    generally does not go through all the data: approximating the
    data using the regression line entails some error.
    As discussed in chapter
<script language="JavaScript1.4" type="text/javascript"><!--
    document.writeln(citeLinkChapter('regressionDiagnostics') + ',' );
// -->
</script>
    the vertical amount by which the line misses a datum is called a
    <span class="termOfArt">residual</span>&mdash;it is the error in estimating the value of
    Y for that datum from its value of X using the regression line.
    The <span class="termOfArt">rms</span> of the residuals has a simple relation to the
    <span class="termOfArt">correlation coefficient</span>
    and the <span class="termOfArt">SD</span> of Y:
    It is <span class="math"><!--(1-r<sup>2</sup>)<sup>&frac12;</sup>&times;SD(Y)-->
              \(\sqrt{1-r^2} \times SD(Y)\) </span> .
</p>

<p>
        There are common mistakes in interpreting regression, including the
        <span class="termOfArt">regression fallacy</span>
        and fallacies related to <span class="termOfArt">ecological correlation</span>, discussed below.
</p>


<h2><a id="rms_error_of_regression"></a>
   The RMS Error of Regression
</h2>

<p class="video"> <iframe width="420" height="315" src="http://www.youtube.com/embed/VsF3MsUUu3E?start=990&end=1645" frameborder="0" allowfullscreen></iframe>
</p>

<p>
    <a id="rms_error_of_regression"></a>
    The <a class="glossRef" href="gloss.htm#regression">regression line</a> does not pass
    through all the data points
    on the <a class="glossRef" href="gloss.htm#scatterplot">scatterplot</a> exactly unless the
    <a class="glossRef" href="gloss.htm#correlation_coef">correlation coefficient</a> is &plusmn;1.
    In general, the data are scattered around the regression line.
    Each datum will have a vertical residual
    from the regression line; the sizes of the vertical residuals will vary from datum to
    datum.
    The rms of the vertical residuals measures the typical vertical distance of a datum from
    the regression line.
</p>

<div class="indent">
<p class="inline">
    Recall that the <a class="glossRef" href="gloss.htm#rms">rms</a> is a measure of
    the typical size of elements in a list. Thus the <a class="glossRef" href="gloss.htm#rms">rms</a> of the
    vertical <a class="glossRef" href="gloss.htm#residual">residuals</a> is a measure of the typical vertical
    distance from the data to the <a class="glossRef" href="gloss.htm#regression">regression line</a>, that is,
    the typical error in estimating the value of Y by the height of the regression line.
    A bit of algebra<script language="JavaScript1.4" type="text/javascript"><!--
    var fStr = 'Let the data be pairs ' +
           '<span class="math">\\( (x_i y_i), i=1, \\dots n \\)</span>. ' +
           'The vertical residual of the<span class="math">\\(i^{th}\\)</span> point is</p>' +
           '<p class="math">\\( y_i-(predicted\\; y_i) = y_i-(\\frac{r \\times SD_Y}{SD_X} \\times x_i + mean(Y) - \\frac{r \\times SD_Y}{SD_X} \\times mean(X)) \\) </p>   ' +
           '<p class="math">\\( = (y_i - mean(Y)) -  \\frac{r \\times SD_Y}{SD_X} \\times (x_i - mean(X)) \\)</p>' +
           '<p> The square of the <span class="math">\\(i^{th}\\)</span> vertical ' +
           'residual is </p> <p class="math">\\( [y_i - (predicted\\; y_i) ]^2 = \\left((y_i - mean(Y)) - ' +
           '\\frac{r \\times SD_Y}{SD_X} \\times (x_i - mean(X))\\right)^2 \\)</p><p class="math"> ' +
           '\\( = (y_i - mean(Y))^2 - 2 \\times r \\times \\frac{SD_Y}{SD_X} \\times (x_i - mean(X)) \\times (y_i - mean(Y)) + \\left(r \\times \\frac{SD_Y}{SD_X} \\times (x_i - mean(X))\\right)^2 \\). </p> <p> We want the sum of those    ' +
           'squares for all <span class="math">\\(n\\)</span> data <span class="math">\\( (i = 1, 2, \\dots , n )\\)</span>. ' +
           'Note that </p> ' +
           '<p class="math"> \\( SD_Y = \\sqrt{\\frac{(y_1 - mean(Y))^2 + (y_2 - mean(Y))^2 + \\dots + (y_n - mean(Y))^2}{n}}\\) </p>' +
           '<p>and</p><p class="math"> \\( r = \\frac{\\left [ \\frac{x_1 - mean(X)}{SD_X} \\times \\frac{y_1 - mean(Y)}{SD_Y} \\right ] + \\left [\\frac{x_2 - mean(X)}{SD_X} \\times \\frac{y_2 - mean(Y)}{SD_Y} \\right ] + ' +
           '\\dots + \\left [\\frac{x_n - mean(X)}{SD_X} \\times \\frac{y_n - mean(Y)}{SD_Y} \\right ]}{n} \\), </p> ' +
           '<p> so </p> <p class="math">\\( (x_1 - mean(X)) \\times (y_1 - mean(Y)) +  (x_2 - mean(X)) \\times (y_2 - mean(Y)) + \\dots + (x_n - mean(X)) \\times (y_n - mean(Y)) =  r \\times n \\times SD_X \\times SD_Y \\).    ' +
           '</p> <p> Now consider the first term in the square of the vertical ' +
           'residuals. Adding the corresponding terms for <span class="math">\\( i = 1, \\dots , n\\)</span>, gives </p> ' +
           '<p class="math">\\( (y_1 - mean(Y))^2 + (y_2 - mean(Y))^2 + \\dots + (y_n - mean(Y))^2 = n \\times (SD_Y)^2 \\). </p> ' +
           '<p> Similarly, the sum of the second terms for <span class="math">\\( i = 1, \\dots , n\\)</span>, ' +
           'gives</p> <p class="math">\\( - 2 \\times r \\times \\frac{SD_Y}{SD_X} \\times [(x_1 - mean(X)) \\times (y_1 - mean(Y)) + (x_2 - mean(X)) \\times (y_2 - mean(Y)) + \\dots + (x_n - mean(X)) \\times (y_n -  mean(Y))] \\)</p>' +
           '<p class="math">\\( = - 2 \\times (SD_Y)^2 \times r \times n  \times r = - 2 \\times n \\times r^2 \\times (SD_Y) ^2\\). </p> <p>The sum of the third terms for <span class="math">\\( = 1, \\dots , n\\)</span>, gives</p>' +
           '<p class="math"> \\( (r \\times \\frac{SD_Y}{SD_X})^2 \\times [(x_1 - mean(X)))^2 + (x_2 - mean(X)))^2 + \\dots + (x_n - mean(X)))^2]  = (r \\times \\frac{SD_Y}{SD_X})^2 \\times n \\times (SD_X)^2 = n \\times r^2 \\times (SD_Y)^2 \\). </p> ' +
           '<p> The sum of the squares of the vertical residuals is thus </p> <p class="math">\\( n \\times (SD_Y)^2 - 2 \\times n \\times r^2 \\times (SD_Y)^2 + n \\times r^2 \\times (SD_Y)^2 = n  \\times (SD_Y)^2 \\times  (1 - 2 \\times r^2 + r^2) =  n  \\times (SD_Y)^2 \\times  (1 - r^2)\\).</p>' +
           '<p>To get from the sum of the squares of  ' +
           'the vertical residuals to the rms of the vertical residuals, we divide  ' +
           'by <span class="math">\\(n\\)</span> and take the square-root:</p> ' +
           '<p class="math">\\( rms(vertical\\;\\; residuals) = \\sqrt{n \\times (SD_Y)^2 \\times \\frac{1 - r^2}{n}} ' +
           ' = SD_Y \\times \\sqrt{1 - r^2} \\).</p> ';
    writeFootnote(fCtr++, fCtr.toString(), fStr);
// -->
</script>
    shows that the
    <a class="glossRef" href="gloss.htm#rms">rms</a>
    of the vertical <a class="glossRef" href="gloss.htm#residual">residuals</a>
    from the <a class="glossRef" href="gloss.htm#regression">regression line</a>
    (<em>the rms error of regression</em>) is
</p>
</div>

<p class="math">
	<!-- (1 &minus; r<sup>2</sup>)<sup>1/2</sup>&times;SD<sub>Y</sub>. -->
	\(\sqrt{1-r^2} \times SD_Y\)
</p>

<p>
    The rms error of regression is always between 0 and SD<sub>Y</sub>.
    It is zero when <span class="math"><!-- r = &plusmn;1 and SD<sub>Y</sub> -->
    \( r = \pm 1 \)</span> and <span class="math">\( SD_Y \)</span> when <span class="math">\(r = 0\)</span>.
    (Try substituting <span class="math">\(r = 1\)</span> and <span class="math">\(r = 0\)</span> into the expression above.)
    When <span class="math"><!--r = &plusmn;1-->\( r = \pm 1 \)</span>,
    the regression line accounts for all of the variability of Y, and the
    <a class="glossRef" href="gloss.htm#rms">rms</a> of the vertical residuals is zero.
    When <span class="math">r = 0</span>, the
    regression line does not &quot;explain&quot; any of the variability of Y: The regression
    line is a horizontal line at height mean(Y), so the rms of the vertical residuals from the
    regression line is the rms of the deviations of the values of Y from the mean of Y, which
    is, by definition, the SD of Y.
    When <span class="math">r</span> is not zero,
    the regression line accounts for some of the variability of Y, so the scatter
    around the regression line is less than the overall scatter in Y.
</p>

<p>
    If the scatterplot is football-shaped, the
    mean of the values in a thin vertical strip will be about the same as the height of the
    regression line, and the SD of the values in a vertical strip will be about the same as
    the rms (vertical) error of regression. Why?
</p>

<p>
    Recall that the regression line is a smoothed version of the
    <a class="glossRef" href="gloss.htm#graph_of_averages">graph of averages</a>:
    The height of the regression line at the point <span class="math">x</span> is an estimate of the
    average of the values of Y  for individuals whose value of X is close to <span class="math">x</span>.
    If the scatterplot is <a class="glossRef" href="gloss.htm#football_shaped">football-shaped</a>,
    the regression line follows the graph of averages reasonably well:
    In each vertical slice, the deviations of the values of Y from their mean is
    approximately the vertical residuals of those values of Y from the regression line.
    The SD of the values of Y in the slice are thus approximately the rms of the residuals
    in the slice.
    Because football-shaped scatterplots are
    <a class="glossRef" href="gloss.htm#homoscedastic">homoscedastic</a>,
    the SD of the values of Y
    in every vertical slice is about the same, so the rms error of regression is a
    reasonable estimate of the scatter of the values of Y in vertical slices through
    football-shaped scatterplots.
</p>

<p>
    In contrast, when the scatterplot is not football-shaped&mdash;because of
    nonlinearity, heteroscedasticity or outliers&mdash;the rms error of regression
    is not a good measure of the scatter in a &quot;typical&quot;
    vertical slice.
    If a scatterplot is homoscedastic and shows
    <a class="glossRef" href="gloss.htm#nonlinear">nonlinear association</a>, the rms error of
    regression tends to overestimate the scatter in a typical vertical slice: the residuals have
    a contribution from scatter around the average in the slice, and a contribution from the difference
    between the average in the slice and the height of the regression line in the slice.
    Similarly, if a scatterplot is heteroscedastic and shows
    <a class="glossRef" href="gloss.htm#linear">linear association</a>, the rms error
    of regression will overestimate the scatter in some slices and underestimate the
    scatter in other slices.
    If a scatterplot has outliers and is otherwise homoscedastic
    and shows linear association, the rms error of regression will tend to overestimate
    the scatter in slices.
    The strength of linear association affects the size of the
    rms error of regression, but it does not affect whether the rms error of regression
    is a good estimate of the scatter in vertical slices.
</p>

<p>
    The following exercises check your ability to calculate the rms error of
    regression and your understanding of its use as a summary.
    Use
<script language="JavaScript1.4" type="text/javascript"><!--
    citeFig(1);
// -->
</script>
    to solve the following exercise.
</p>

<div class="problem">
<script language="JavaScript1.4" type="text/javascript"><!--
    document.writeln(startProblem(pCtr++));
    var qStr = '<span class="qSpan">What is the rms of the residuals from the regression ' +
           'line for regressing Verbal GMAT on Quantitative GMAT? </span>';
    document.writeln(qStr);
    var res = Math.sqrt(1 - (.35)*(.35))*6.22;
    writeTextExercise(6,qCtr++,numToRange(res,.1));
    var qStr = '</p><p><span class="qSpan">For regressing Quantitative GMAT on Verbal ' +
           'GMAT?</span>';
    document.writeln(qStr);
    var res2 = Math.sqrt(1 - (.35)*(.35))*6.77;
    writeTextExercise(6,qCtr++,numToRange(res2,.1));
    document.writeln('</p>');
    var ansStr = 'The rms of the residuals for regressing of Verbal GMAT (VGMAT) on ' +
         'Quantitative GMAT (QGMAT) is </p><p class="math">' +
         '\( \left(1 - r^2\right)^\frac{1}{2} \times SD(VGMAT) = \left(1 - 0.35^2\right)^\frac{1}{2} \times 6.22 = ' +
         roundToDig(res,3) + '\). </p><p>Similarly, the rms of the residuals for regressing ' +
         'QGMAT on VGMAT is </p><p class="math">' +
         '\( \left(1 - r^2\right)^\frac{1}{2} \times SD(QGMAT) = \left(1 - 0.35^2\right)^\frac{1}{2} \times 6.77 = ' +
         roundToDig(res2,3) + '\). ';
    writeSolution(pCtr-1, ansStr);
// -->
</script>
</div>

<div class="problem">
<script language="JavaScript1.4" type="text/javascript"><!--
    document.writeln(startProblem(pCtr++));
// this exercise tests computing the rms error of regression, and inferring
// from a scatterplot whether or not it is a good summary of the scatter in vertical strips.
//
// Fake weight and height data. Take mean weight = 150#; mean height = 66". Slope 1"/5# = .2.
// ratio of SDs 6"/40# = .15
//
    var examplePoints = 100;
    var slope = 0.2;
    var htRg0 = 20;
    var aveWt0 = 150;
    var aveHt0 = 67;
    var sdRat = 0.15
    var pVals = listOfRandInts(2,-1,1);
    var parms = [["polynomial", aveHt0, slope,.04*Math.abs(pVals[0])],
        1 + 4*Math.max(pVals[1],0), 10];
    var data6 = fakeBivariateData(examplePoints,parms[0],parms[1],parms[2], 0, 80);
    var htMnMx = vMinMax(data6[1]);
    var loHt = aveHt0 - htRg0/2;
    var htRg = htMnMx[1] - htMnMx[0];
    for (i=0; i < data6[0].length; i++) {
    data6[0][i] += aveWt0 - 40;
    data6[1][i] = loHt + (data6[1][i] - htMnMx[0])*htRg0/htRg;
    }
    var cc = roundToDig(corr(data6[0],data6[1]),2);
    var aveWt = roundToDig(mean(data6[0]),2);
    var aveHt = roundToDig(mean(data6[1]),2);
    var sdWt =  roundToDig(sd(data6[0]),2);
    var sdHt =  roundToDig(sd(data6[1]),2);
    var rmsResid = Math.sqrt(1-cc*cc)*sdHt;
    var qStr = 'The scatterplot below shows fabricated data for the ' +
           'heights and weights of 100 individuals. The average weight is ' +
           aveWt.toString() + ' lbs. with an SD of ' + sdWt.toString() +
           ' lbs., and the average height is ' + aveHt.toString() +
           'in. with an SD of ' + sdHt.toString() + ' in. The correlation between ' +
           'height and weight is ' + cc.toString() +
           '. <span class="qSpan">The rms error of regression for regressing height ' +
           'against weight is</span>';
    document.writeln(qStr);
    writeTextExercise(6,qCtr++,numToRange(rmsResid,.04));
    var opt = ["inches.","pounds.","(no units)."];
    writeSelectExercise(false,qCtr++,opt,'a');
    var qStr = '</p><p><span class="qSpan">For this scatterplot, the rms error of ' +
           'regression is a</span>';
    document.writeln(qStr);
    var opt=["good","poor"];
    var aVal;
    if (pVals[0] != 0 || pVals[1] > 0) {
    aVal = 'b';
    } else {
    aVal  = 'a';
    }
    writeSelectExercise(false,qCtr++,opt,aVal);
    var qStr = '<span class="qSpan">measure of the scatter in a typical vertical strip, ' +
           'because of</span>';
    document.writeln(qStr);
    var opt = ["weak association","nonlinear association","heteroscedasticity",
        "outliers","none of the above"];
    var aVal = "";
    var none = true;
    if (pVals[0] != 0) {
    aVal += "b&";
    none = false;
    }
    if (pVals[1] > 0) {
    aVal += "c&";
    none = false;
    }
    if (none) aVal += "e";
    if (aVal.lastIndexOf('&') == aVal.length - 1) aVal = aVal.substring(0, aVal.length - 1);
    writeSelectExercise(true,qCtr++,opt,aVal);
    var qStr = '</p><p class="figure"><applet code="Correlation.class" ' +
           'codebase="../../Java/" align="baseline" width="600" height="320" ' +
           'name="q6Applet" archive="PbsGui.zip"><param name="addPoints" value="false">' +
           '<param name="graphAveButton" value="false">' +
           '<param name="sdButton" value="false">' +
           '<param name="sdLineButton" value="false">' +
           '<param name="title" value="Fake Weight v. Height Data">' +
           '<param name="regressButtons" value="true">' +
           '<param name="showR" value="true">' +
           '<param name="x" value="1,2,4"><param name="y" value="1,3,5">' +
           'You need Java to see this.</applet></p>';
    document.writeln(qStr);
    var appNum = (document.applets.length - 1).toString();
    sectionContext += 'document.applets[' + appNum + '].' +
              'setVariables(data6[0].join(),data6[1].join() );\n' +
              'document.applets[' + appNum + '].showPlot();\n';
// -->
</script>
</div>

<h2>
  <a id="dist_in_slices"></a>The Distribution of Data in
  Slices through a Scatterplot
</h2>

<p>
    The values of Y for a given value of X
    (or a small range of values of X)  have a distribution that typically
    differs from the overall distribution of the values of Y without regard for the value of X.
    Therefore, the mean of the values of Y in such a slice typically differs from the overall mean of Y,
    and the SD of the values of Y in a slice typically differs from the overall SD of Y.
</p>

<p>
<script language="JavaScript1.4" type="text/javascript"><!--
    citeFig();
// -->
</script>
    lets us superpose the histogram of a variable for all
    individuals with the histogram of that variable just for those
    individuals whose value of that or another variable is within a given range&mdash;a
    slice through the scatterplot.
    That is, it allows us to look at the histogram of Y values for all individuals
    in a set of multivariate data, and the histogram of Y values for only
    those individuals who have X values in a specified range.
    We shall look at the GMAT data.
</p>

<div class="figure">
<script language="JavaScript1.4" type="text/javascript"><!--
    var qStr = 'Superposed Histograms of GMAT Data with and without Restrictions';
    writeFigureCaption(qStr);
// -->
</script>

<p class="figure">
    <applet code="HistHiLite.class" codebase="../../Java/" align="middle"
       width="620" height="320" archive="PbsGui.zip">
    <param name="bins" value="50">
    <param name="files" value="./Data/gmat.dat">
    <param name="normalControls" value="false">
    <param name="restrict" value="true">
    <param name="univariateStats" value="true">
    You need Java to see this.
    </applet>
</p>
</div>

<p>
    We first superposed histograms to study association in
<script language="JavaScript1.4" type="text/javascript"><!--
    document.writeln(citeLinkChapter('scatterplots') + '. ');
// -->
</script>
    This applet should display the verbal GMAT scores when you first visit this page.
    If not, select &quot;Verbal&quot; from the <span class="appCtrlName">Variable</span> drop-down menu.
    Use the <span class="appCtrlName">Restrict to</span> drop-down menu to select Quantitative GMAT.
    The <a class="glossRef" href="gloss.htm#mean">mean</a> of the values of Verbal GMAT scores
    for just those individuals whose
    Quantitative GMAT scores are in a restricted range is typically different
    from the mean of the Verbal GMAT scores for all individuals;
    The <a class="glossRef" href="gloss.htm#sd">SD</a> of the restricted set of Verbal GMAT
    scores is also typically different
    from the overall SD of the Verbal GMAT scores.
    The SD is a measure of their spread, and in the case of
    <a class="glossRef" href="gloss.htm#football_shaped">football-shaped</a> scatterplots,
    is about the same as the
    <a class="glossRef" href="gloss.htm#rms_error_of_regression">rms error of regression</a>.
    We can use what we know about univariate distributions to calculate properties
    of the distribution of values of verbal GMAT corresponding to a given value
    of quantitative GMAT.
</p>

<p>
    If the correlation coefficient <span class="math">r</span> is positive and the data are
    homoscedastic, individuals with a given value of X that
    is above the mean of X are a subset of the population
    that tends to have larger than average values of Y; and the scatter of those
    values tends to be less than the scatter of Y for the entire population,
    by the factor <span class="math"><!--(1 &minus; r<sup>2</sup>)<sup>&frac12;</sup>-->
        \( \sqrt{1 - r^2} \)</span>.
    Individuals with a value of X that is smaller than the mean of X are a
    subset of the population that tends to have smaller than average values of Y;
    and the scatter of those values tends to be less than the overall scatter of
    Y for the entire population.
    The same thing holds for negative correlation, <em>mutatis mutandis</em>.
</p>

<h2>
   <a id="regression_effect"></a>
   The Regression Effect
</h2>

<p class="video">
   <iframe width="420" height="315" src="http://www.youtube.com/embed/VsF3MsUUu3E?start=1655&end=2520" frameborder="0" allowfullscreen></iframe>
</p>

<p>
    In
<script language="JavaScript1.4" type="text/javascript"><!--
    document.writeln(citeLinkChapter('regression') + ', ');
// -->
</script>
    we saw that for football-shaped
    scatterplots the graph of averages is not as steep as the SD line,
    unless <span class="math"><!--r = &plusmn;1-->\( r = \pm 1 \)</span>: If
    <span class="math"><!--0 &lt; r &lt; 1-->\( 0 < r < 1 \)</span>, the average value of
    \(Y\) for individuals whose values of \(X\) are about <span class="math"><!--kSD<sub>X</sub>-->\(kSD_X\)</span>
    above the mean of \(X\) is less than <span class="math"><!--kSD<sub>Y</sub>-->\(kSD_Y\)</span> above the mean(Y).
    Similarly, if <span class="math"><!--&minus;1 &lt; r &lt; 0-->\( -1 < r < 0 \)</span>, the average value of
    \(Y\) for individuals whose
    values of \(X\) are about <span class="math"><!--kSD<sub>X</sub>-->\( kSD_X \)</span> above \(mean(X)\) is less than
    <span class="math"><!--kSD<sub>Y</sub>-->\( kSD_Y \)</span> below \(mean(Y)\).
</p>

<p>
    This phenomenon is called the <span class="termOfArt">regression effect</span> or
    <span class="termOfArt">regression towards the mean</span>.
    Individuals with a given value of X tend to have values of Y that are closer
    to the mean, where closer means fewer SD away.
    Consider the IQs of a large group of married couples.
    Essentially by definition, the average IQ score is 100.
    The SD of IQ is about 15 points. Suppose that for this group,
    the correlation between the IQs of spouses is 0.7&mdash;women with above average
    IQ tend to marry men with above average IQ, and vice versa.
    Consider a woman in the group whose IQ is 150 (genius level).
    What is our best estimate of her husband's IQ?
    We shall estimate his IQ  using the regression line: Her IQ is 150,
    which is 50 points above average. 50 points is
</p>
<p class="math">
    <!--(3 <small>1/3</small>)&times;15points
    = 3 <small>1/3</small> SD,-->
    \( 3 \tfrac{1}{3} \times 15 points = 3 \tfrac{1}{3} \)
</p>

<p>
    so we would estimate the husband's IQ to be
    <span class="math"><!--r&times;3<small>1/3</small> SD = 0.7&times;3 <small>1/3</small> SD-->
    \( r \times 3\tfrac{1}{3} SD = 0.7 \times 3\tfrac{1}{3} SD \)</span>
    above average, or about
    <span class="math"><!--2 <small>1/3</small> SD-->\( 2\tfrac{1}{3} SD \)</span> above average.
    Now <span class="math"><!--2 <small>1/3</small> SD-->\( 2\tfrac{1}{3} SD \)</span> is 35 points,
    so we expect the husband's IQ to be about 135, not nearly as &quot;smart&quot; as she is.
</p>

<p>
    Now let's predict the IQ of the wife of a man whose IQ is 135.
    His IQ is <span class="math"><!--2 <small>1/3</small> SD-->\( 2\tfrac{1}{3} SD \)</span>
    above average, so we expect her IQ to be
    <span class="math"><!--0.7&times;2 <small>1/3</small>
    SD-->\( 0.7 \times 2\tfrac{1}{3} SD \)</span> above average. That's about 1.63 SD or <!--1.63&times;15 = 24&frac12;--> \( 1.63 \times 15 = 24\tfrac{1}{2} \)
    points above average, or <!--124&frac12;-->\( 124\tfrac{1}{2} \),
    not as &quot;smart&quot; as he is.
    How can this be consistent?
</p>

<p>
    The algebra is correct.
    The phenomenon is quite general.
    It is called the <a class="glossRef" href="gloss.htm#regression_effect">regression
    effect</a>.
    The regression effect is caused by the same thing that makes the slope of the
    regression line smaller in magnitude than the slope of the SD line.
    If the scatterplot is
    football-shaped and <span class="math">r</span> is at least zero but less than 1, then
</p>

<ul>
    <li>
        In a vertical slice containing above-average values of X, most of the y
        coordinates are below the SD line.
    </li>
    <li>
        In a vertical slice containing below-average values of X, most of the y
        coordinates are above the SD line.
    </li>
</ul>

<p>
    If the scatterplot is football-shaped and <span class="math">r</span> is less than zero
    but greater than &minus;1:
</p>

<ul>
    <li>
        In a vertical slice for above-average values of X, most of the y
        coordinates are above the SD line.
    </li>
    <li>
        In a vertical slice for below-average values of X, most of the y
        coordinates are below the SD line.
    </li>
</ul>

<p>
     Only if <span class="math">r</span> is &plusmn;1 does the regression line estimate the value
     of Y to be as many SDs from the mean as the value of X is; otherwise, the
     regression line estimates the value of Y to be fewer SDs from the mean.
     If <span class="math">r</span> is positive but less than 1, the regression line estimates
     Y to be above its mean if X is above its mean, but by fewer SDs.
     If <span class="math">r</span> is negative but greater than &minus;1, the regression line estimates Y
     to be below its mean if X is above its mean, but by fewer SDs than X is
     above its mean.
     This is another way of expressing the regression effect.
</p>

<p>
    The regression effect does not say that an individual who is a given number of
    SD from average in one variable <em>must</em> have a value of the other variable
    that is closer to average&mdash;merely that individuals who are a given number of
    SD from the mean in one variable tend on average to be fewer SD from the
    mean in the other.
</p>

<p>
    In most test/re-test situations, the correlation between scores on the test
    and scores on the re-test is positive, so individuals who score much higher
    than average on one test tend to score above average, but closer to average,
    on the other test.
    (In the previous example, &quot:individuals&quot; are couples, the first
    test is the IQ of one spouse, and the second test is the IQ of the other spouse.)
    Similarly, individuals who are much lower than average in one variable tend to
    be closer to average in the other (but still below average).
    Those who perform best usually do so with a combination of skill (which will be
    present in the retest) and exceptional luck (which will likely not be so good
    in a retest).
    Those who perform worst usually do so as the result of a
    combination of lack of skill (which still won't be present in a retest) and
    bad luck (which is likely to be better in a retest).
    If the scatterplot is football-shaped, many more individuals are near the
    mean than in the tails.
    A particularly high score could have come from someone with an even higher
    true ability, but who had bad luck, or someone with a lower true ability
    who had good luck. Because more individuals are near average,
    the second case is more likely; when the second case occurs on a retest,
    the individual's luck is just as likely to be bad as good, so the
    individual's second score will tend to be lower.
    The same argument applies, <em>mutatis mutandis</em>, to the case of a
    particularly low score on the first test.
</p>

<p>
    The regression effect does not require the second score to be less extreme
    than the first: nothing prevents an individual from have a score that is
    even more extreme on the second test.
    The regression effect describes what happens on the average.
</p>

<p>
    <a id="regression_fallacy"></a>Failing to account for the
    <a class="glossRef" href="gloss.htm#regression_effect">regression effect</a>, concluding that something must
    cause the difference in scores, is called the
    <a class="glossRef" href="gloss.htm#regression_fallacy">regression
    fallacy</a>. The <a class="glossRef" href="gloss.htm#regression_fallacy">regression fallacy</a> sometimes
    leads to amusing mental gymnastics and speculation, but can also be pernicious.
</p>

<p>
    <strong>Example: Pilot training in the Israeli Airforce</strong>.
    (From <a href="references.htm#tversky_kahneman" target="_self">Tversky and
    Kahneman, 1974</a>.)
    The Israeli Airforce performed a study to determine the effectiveness of punishment and
    reward on pilot training.
    Some students were
    praised after particularly good landings, and others were reprimanded after particularly
    bad ones.
    Students who were praised usually did worse on their next
    landing, while those who were reprimanded usually did better on their next landing.
    The obvious conclusion is that reward hurts, and punishment helps.
</p>

<p>
    How might this be an instance of the regression fallacy?
    After a particularly bad landing, one would expect the next to be closer to average,
    whether or not the student is reprimanded. Similarly, after a particularly good landing,
    one would expect the next to be closer to average, whether or not the student is praised.
</p>

<p>
    The following exercise checks your understanding of the regression effect.
</p>

<div class="problem">
<script language="JavaScript1.4" type="text/javascript"><!--
    document.writeln(startProblem(pCtr++));
    var ch = listOfRandInts(2,0,1);
    var qStr = 'Consider a large statistics class. <span class="qSpan">Because of the ' +
           'regression effect, the student who scores ';
    if (ch[0] == 0) {
       qStr += ' lowest ';
    } else {
       qStr += ' highest ';
    }
    qStr += ' on the midterm will ';
    if (ch[1] == 0) {
       qStr += ' tend to score ';
    } else {
      qStr += ' score ';
    }
    if (ch[0] == 0) {
       qStr += ' higher on the final.';
    } else {
       qStr += ' lower on the final.';
    }
    qStr += '</span>';
    document.writeln(qStr);
    var opt = ["false","true"];
    var aVal;
    if (ch[1] == 0) {
       aVal = 'b';
    } else {
       aVal = 'a';
    }
    writeSelectExercise(false,qCtr++,opt,aVal);
    document.writeln('</p>');
// -->
</script>
</div>

<h2>
    <a id="summary"></a>Summary
</h2>

<p>
    The rms of the residuals, also called the rms error of regression,
    measures the average error of the regression line in estimating the
    dependent variable Y from the independent variable X.
    The rms error of regression depends only on the correlation
    coefficient of X and Y and the SD of Y:
</p>

<p class="math">
    <!-- rms error of regression = (1 &minus; (r<sub>XY</sub>)<sup>2</sup>)<sup>&frac12;</sup>
    &times;SD<sub>Y</sub>-->
    \( \mbox{rms error of regression} = \left(1 - (r_{XY})^2\right)^\tfrac{1}{2} \times SD_Y \)
</p>

<p>
    If the correlation coefficient is <!--&plusmn;1-->\(\pm1\), the rms error of regression is zero:
    The regression line passes through all the data.
    If <span class="math">r = 0</span>, the rms error of regression is <!--SD<sub>Y</sub>-->\( SD_Y \):
    The regression line estimates Y no better than the mean of Y does&mdash;in fact,
    when <span class="math">r = 0</span> the regression line is a horizontal line whose
    intercept is the mean of Y.
</p>

<p>
    For football-shaped scatterplots, unless <span class="math"><!--r = &plusmn;1-->\(r = \pm 1\)</span>
    the graph of averages
    is not as steep as the SD line: The average of Y in a vertical slice is fewer
    SDs from the mean than the value of X that defines the slice.
    The regression line estimates the value of the dependent variable to be
    fewer SDs from the mean than the value of the independent variable.
    This is called the regression effect, or regression towards the mean.
    The regression line estimates the value of the dependent variable to be
    on the same side of the mean as the value of the independent variable if
    <span class="math">r</span>
    is positive, and on the opposite side of the mean if <span class="math">r</span> is negative.
    (If <span class="math">r = 0</span>, it estimates that the value of the dependent variable
    will equal the mean.)
    Ignoring the regression effect leads to the regression fallacy:
    inventing extrinsic causes for phenomena that are explained adequately by
    the regression effect.
</p>

<h2><a id="keyTerms"></a>Key Terms</h2>

<ul>
    <li>correlation coefficient</li>
    <li>dependent variable</li>
    <li>football-shaped</li>
    <li>graph of averages</li>
    <li>heteroscedasticity</li>
    <li>histogram</li>
    <li>homoscedastic</li>
    <li>independent variable</li>
    <li>mean</li>
    <li>mutatis mutandis</li>
    <li>nonlinear</li>
    <li>nonlinearity</li>
    <li>outlier</li>
    <li>percentile</li>
    <li>regression effect</li>
    <li>regression fallacy</li>
    <li>regression line</li>
    <li>residual</li>
    <li>residual plot</li>
    <li>rms</li>
    <li>rms error of regression</li>
    <li>scatterplot</li>
    <li>SD</li>
    <li>slice</li>
    <li>variable</li>
</ul>

</form>

<script language="JavaScript1.4" type="text/javascript"><!--
    writeChapterFooter();
// -->
</script>
</body>
</html>
